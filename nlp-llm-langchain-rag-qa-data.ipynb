{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3203681,"sourceType":"datasetVersion","datasetId":1143836}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yannicksteph/nlp-llm-langchain-rag-qa-data?scriptVersionId=157432537\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# | NLP | LLM | LangChain RAG | QA Data |\n\n## Natural Language Processing (NLP) and Large Language Models (LLM) with LangChain and Inretrieval augmented generation (RAG) for Question Answering on Own Data\n\n![Learning](https://t3.ftcdn.net/jpg/06/14/01/52/360_F_614015247_EWZHvC6AAOsaIOepakhyJvMqUu5tpLfY.jpg)\n\n\n# <b>1 <span style='color:#78D118'>|</span> Overview</b>\n\nIn this notebook we're going to augment the knowledge base of our LLM with additional data:\n- We will walk through how to load data, local text file using a `DocumentLoader`, split it into chunks, and store it in a vector database using `ChromaDB`.\n- And using Question Answering on Own Data \n\nInretrieval augmented generation (RAG) framework, an LLM retrieves contextual documents from an external dataset as part of its execution. This is useful when we want to ask questions about specific documents (e.g., PDFs, videos, etc). \n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_1.png?raw=true\" alt=\"Learning\" width=\"100%\">\n\n\n## Learning Objectives\n\n By the end of this notebook, you will be able to:\n1. Add external local data to your LLM's knowledge base via a vector database.\n2. Construct a Question-Answer(QA) LLMChain to \"talk to your data.\"\n3. Load external data sources from remote locations and store in a vector database.\n4. Leverage different retrieval methods to search over your data. \n\n<img src=\"https://deepsense.ai/wp-content/uploads/2023/10/LangChain-announces-partnership-with-deepsense.jpeg\" alt=\"Learning\" width=\"50%\">\n\n\n[Using-langchain-for-question-answering-on-own-data](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Setup\n","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Fill\"","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:43:16.610808Z","iopub.execute_input":"2024-01-02T14:43:16.611185Z","iopub.status.idle":"2024-01-02T14:43:16.621917Z","shell.execute_reply.started":"2024-01-02T14:43:16.611155Z","shell.execute_reply":"2024-01-02T14:43:16.620841Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n!pip install chromadb==0.4.10 tiktoken==0.3.3 sqlalchemy==2.0.15\n!pip install langchain==0.0.249\n!pip install --force-reinstall pydantic==1.10.6 \n!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:43:16.62371Z","iopub.execute_input":"2024-01-02T14:43:16.624026Z","iopub.status.idle":"2024-01-02T14:44:36.734206Z","shell.execute_reply.started":"2024-01-02T14:43:16.623996Z","shell.execute_reply":"2024-01-02T14:44:36.733162Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, ConversationalRetrievalChain, ConversationChain\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.schema import messages_from_dict, messages_to_dict\nfrom langchain.memory.chat_message_histories.in_memory import ChatMessageHistory\nfrom langchain.agents import Tool\nfrom langchain.agents import initialize_agent\nfrom langchain.agents import AgentType","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:44:36.73552Z","iopub.execute_input":"2024-01-02T14:44:36.735822Z","iopub.status.idle":"2024-01-02T14:44:39.175914Z","shell.execute_reply.started":"2024-01-02T14:44:36.735795Z","shell.execute_reply":"2024-01-02T14:44:39.174994Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"cache_dir = \"./cache\"","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:44:39.178058Z","iopub.execute_input":"2024-01-02T14:44:39.178478Z","iopub.status.idle":"2024-01-02T14:44:39.182562Z","shell.execute_reply.started":"2024-01-02T14:44:39.178451Z","shell.execute_reply":"2024-01-02T14:44:39.181593Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_column', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_seq_items', None)\npd.set_option('display.max_colwidth', 500)\npd.set_option('expand_frame_repr', True)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:44:39.183535Z","iopub.execute_input":"2024-01-02T14:44:39.183887Z","iopub.status.idle":"2024-01-02T14:44:39.489466Z","shell.execute_reply.started":"2024-01-02T14:44:39.18385Z","shell.execute_reply":"2024-01-02T14:44:39.48868Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:#78D118'>|</span> Question-Answer(QA) LLMChain to \"talk to your data.\"</b>\n\nWe're going to build a special type of LLMChain that will enable us to ask questions of our data. We will be able to \"speak to our data\".\n\n### Step 1 - Document Loading\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_2.png?raw=true\" alt=\"Learning\" width=\"30%\" style=\"opacity: 0.7;\">\n\nDocument Loading in the Inretrieval Augmented Generation (RAG) framework is a critical step where a Language Model (LLM) retrieves contextual documents from external datasets during execution. This process is particularly valuable when seeking to ask questions about specific documents, such as PDFs or videos. To enable applications to interact with this data through chat interfaces, it's necessary to load the data into a format that facilitates processing.\n\nLangChain's Retrieval Augmented Generation (RAG) employs document loaders to handle the intricacies of accessing and converting data from various formats and sources into a standardized format. This includes structured and unstructured data sources such as websites, databases, YouTube, arxiv, Twitter, Hacker News, as well as proprietary sources like Figma, Notion, Airbyte, Stripe, and Airtable. These loaders accommodate diverse data types like pdf, html, json, word, PowerPoint, or tabular formats, transforming them into standard document objects with content and metadata.\n\nLangChain boasts over 80 different document loaders, including PyPDF DataLoader for loading PDFs, Youtube DataLoader for handling YouTube videos, WebBaseLoader for loading URLs from the internet, and NotionDirectoryLoader for retrieving data from Notion. Each loader produces a list of documents, where each document contains page content and associated metadata. For large documents, the capability to split them into smaller chunks is essential, especially in the context of retrieval augmented generation, where relevance is crucial.\n\nTo show how well we can scale the vector database, let's load in a larger document. For this we'll get data from the Gutenberg Project with thousands of free-to-access texts. We'll use the complete works of William Shakespeare.\n\nInstead of a local text document, we'll download the complete works of Shakespeare using the GutenbergLoader that works with the Gutenberg project: https://www.gutenberg.org\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-29T10:22:36.998626Z","iopub.execute_input":"2023-12-29T10:22:36.99952Z","iopub.status.idle":"2023-12-29T10:22:37.004082Z","shell.execute_reply.started":"2023-12-29T10:22:36.999482Z","shell.execute_reply":"2023-12-29T10:22:37.002916Z"}}},{"cell_type":"code","source":"from langchain.document_loaders import GutenbergLoader\n\nloader = GutenbergLoader(\n    \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n)\n\ndocument = loader.load()\n\nextrait = ' '.join(document[0].page_content.split()[:100])\ndisplay(extrait + \" .......\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:44:39.490554Z","iopub.execute_input":"2024-01-02T14:44:39.491009Z","iopub.status.idle":"2024-01-02T14:44:40.489158Z","shell.execute_reply.started":"2024-01-02T14:44:39.490982Z","shell.execute_reply":"2024-01-02T14:44:40.48824Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"'The Project Gutenberg eBook of The Complete Works of William Shakespeare This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: The Complete Works of William Shakespeare .......'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 2 - Document Splitting\n\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_3.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\n\nDocument Splitting in LangChain is a crucial process that involves breaking down documents into smaller, semantically relevant chunks. This is essential for maintaining meaningful relationships between the chunks, ensuring accurate information retrieval. To achieve this, LangChain employs RecursiveCharacterTextSplitter and CharacterTextSplitter, each with specified chunk sizes and overlaps. The splitters use different methods, such as character or token-based splitting, and consider metadata, language specifics, and document types.\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_4.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\nLangChain offers various types of text splitters, each with methods for creating and splitting documents. These methods share logic but differ in chunking strategies, length measurement, and handling metadata. Examples include Recursive Text Splitter and Character Text Splitter, demonstrated with toy use cases.\n\nReal-world examples showcase the effectiveness of LangChain's splitters in handling different scenarios. RecursiveCharacterTextSplitter and CharacterTextSplitter are illustrated using examples of text and PDF splitting. The splitters handle separators like spaces, newlines, and custom delimiters, ensuring accurate and context-aware chunking.\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_5.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\nAdditionally, LangChain provides TokenTextSplitter for token-based splitting, considering the token count, which is beneficial for language models with token-specific context windows. The context-aware splitting also extends to MarkdownHeaderTextSplitter, preserving header metadata during chunking.\n\nUltimately, LangChain's Document Splitting prepares data for storage in a vector store, enhancing the efficiency of information retrieval.\n\n","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\nimport tempfile\n\n# Chunk sizes of 1024 and an overlap of 256 (this will take approx. 10mins with this model to build our vector database index)\ntext_splitter = CharacterTextSplitter(\n    chunk_size=1024, \n    chunk_overlap=256\n) \ntexts = text_splitter.split_documents(document)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:44:40.490271Z","iopub.execute_input":"2024-01-02T14:44:40.490565Z","iopub.status.idle":"2024-01-02T14:44:41.090872Z","shell.execute_reply.started":"2024-01-02T14:44:40.49054Z","shell.execute_reply":"2024-01-02T14:44:41.089939Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Step 3 - Vector Stores and Embeddings","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_6.png?raw=true\" alt=\"Learning\" width=\"60%\">\n\nVector Store and Embeddings in LangChain play a crucial role in the retrieval augmented generation (RAG) framework. After splitting documents into smaller chunks, LangChain employs embeddings to create numerical representations of text, enabling the comparison of semantically similar content. These embeddings are stored in a vector store, a database facilitating easy retrieval of similar vectors, a process essential for finding relevant documents when answering questions.\n\nWe divided our document into smaller sections and created embeddings for efficient retrieval when answering questions. Using vector stores, we store these embeddings, making it easy to locate similar content. The process involves document splitting, embedding creation, and storing in a vector store.\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_7.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\nThe vector store acts as a database for quick lookup of similar vectors, aiding in finding relevant documents for questions. For question answering, we generate embeddings for the question, compare them with stored vectors, select the most similar ones, and pass them along with the question to a language model to obtain the answer.\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_8.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\n\nWe use Chroma as the vector store, persisting it for future use. Conducting a similarity search, we ask questions and retrieve relevant documents. However, there are edge cases where duplicate results or lack of structured information may affect the search accuracy, as seen in examples provided.\n\n","metadata":{}},{"cell_type":"markdown","source":"Now we'll create embeddings for our document so we can store it in a vector store and feed the data into an LLM. We'll use the sentence-transformers model for out embeddings. https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/\n\n\nFor this system we'll leverage the [ChromaDB vector database](https://www.trychroma.com/) and load in some text we have on file.","metadata":{}},{"cell_type":"code","source":"model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=model_name, \n    cache_folder=cache_dir\n)  # Use a pre-cached model\n\nvectordb = Chroma.from_documents(\n    texts, \n    embeddings, \n    persist_directory=cache_dir\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:44:41.091902Z","iopub.execute_input":"2024-01-02T14:44:41.092167Z","iopub.status.idle":"2024-01-02T14:45:24.407331Z","shell.execute_reply.started":"2024-01-02T14:44:41.092143Z","shell.execute_reply":"2024-01-02T14:45:24.406243Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f33d284c62c4062b2afc0be31e24dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88ac34db0e54af1a5aa2b56b206008a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1451ee11164c8bb773765251ff22d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21259c8b52924754a8c62ae107bb3f8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"845f832f65214c0e85bacf7110131b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9039aec393b6443c9923fc6942baf121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5f103f39dc46f49a314d0e3aad69b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4125b89898746f78249a5cd6a546e29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5103ecb57d314b4d81de00dce06a21dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e611baa77934d4b8462630ada7d2906"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d11060a82ea54ddc83051c5dd35d80a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d5d92ad51f9485c938719f9b5e2515c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5cfff53b4bc4bb88c25bbdaac6bdda2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5d3db3fc884532bc54d06f0aac3911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/243 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e80f6f4f21491ebd1962760e7dca0e"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Similarity Search\nWe will now ask questions using the similarity search method and pass k, which specifies the number of documents that we want to return.","metadata":{}},{"cell_type":"code","source":"question = \"Romeo!\"\n\ndocs = vectordb.similarity_search(question,k=2)\n\n# Check the length of the document\nprint(len(docs))\n\n# Check the content of the first document\nprint(docs[0].page_content)\n\n# Persist the database to use it later\nvectordb.persist()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:45:24.409075Z","iopub.execute_input":"2024-01-02T14:45:24.409579Z","iopub.status.idle":"2024-01-02T14:45:24.449047Z","shell.execute_reply.started":"2024-01-02T14:45:24.409554Z","shell.execute_reply":"2024-01-02T14:45:24.448126Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c038fffd185a43988eae22746a9e12f1"}},"metadata":{}},{"name":"stdout","text":"2\nRomeo! My cousin Romeo! Romeo!\n\n\n\n\n\nMERCUTIO.\n\n\nHe is wise,\n\n\nAnd on my life hath stol’n him home to bed.\n\n\n\n\n\nBENVOLIO.\n\n\nHe ran this way, and leap’d this orchard wall:\n\n\nCall, good Mercutio.\n\n\n\n\n\nMERCUTIO.\n\n\nNay, I’ll conjure too.\n\n\nRomeo! Humours! Madman! Passion! Lover!\n\n\nAppear thou in the likeness of a sigh,\n\n\nSpeak but one rhyme, and I am satisfied;\n\n\nCry but ‘Ah me!’ Pronounce but Love and dove;\n\n\nSpeak to my gossip Venus one fair word,\n\n\nOne nickname for her purblind son and heir,\n\n\nYoung Abraham Cupid, he that shot so trim\n\n\nWhen King Cophetua lov’d the beggar-maid.\n\n\nHe heareth not, he stirreth not, he moveth not;\n\n\nThe ape is dead, and I must conjure him.\n\n\nI conjure thee by Rosaline’s bright eyes,\n\n\nBy her high forehead and her scarlet lip,\n\n\nBy her fine foot, straight leg, and quivering thigh,\n\n\nAnd the demesnes that there adjacent lie,\n\n\nThat in thy likeness thou appear to us.\n\n\n\n\n\nBENVOLIO.\n\n\nAn if he hear thee, thou wilt anger him.\n\n\n\n\n\nMERCUTIO.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 4 - Retrieval in LangChain\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_9.png?raw=true\" alt=\"Learning\" width=\"70%\">\n\nRetrieval is crucial in our retrieval augmented generation (RAG) flow, addressing challenges in question-answering over documents. LangChain introduces advanced mechanisms like Self-query and Contextual Compression for improved retrieval during query time.\n\nExploring advanced retrieval methods:\n1. **Maximum Marginal Relevance (MMR):**\n   - Ensures diversity in search results.\n   - Selects a diverse set of documents, overcoming limitations of semantic search.\n   \n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_10.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\n2. **Metadata Inclusion:**\n   - Addresses specificity in search results.\n   - Uses metadata filters to refine search queries, enhancing result accuracy.\n\n3. **Self Query:**\n   - Leverages language models to extract query string and metadata filters.\n   - Eliminates manual filter specification, enhancing efficiency.\n\n4. **Contextual Compression:**\n   - Improves the quality of retrieved documents.\n   - Extracts relevant segments, reducing the need for expensive language model calls.\n   \n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_11.png?raw=true\" alt=\"Learning\" width=\"50%\">","metadata":{}},{"cell_type":"markdown","source":"### Step 5 - Question Answering\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_12.png?raw=true\" alt=\"Learning\" width=\"70%\">\n\nQuestion Answering with Retrieval:\nLearn how to perform question answering using documents retrieved in the Retrieval phase. Use a language model to answer questions based on both the retrieved documents and the original question.\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_13.png?raw=true\" alt=\"Learning\" width=\"50%\">\n\nRetrievalQA Chain:\nExplore question answering after retrieving relevant splits from the vector store. Compress splits if needed and send them, along with a system prompt and human question, to the language model for answers. Methods like MapReduce, Refine, and MapRerank are available for handling a high number of documents.\n\nRetrieval QA Chain with Methods:\nPass documents into the same context window by default, or use methods like MapReduce, Refine, and MapRerank for high document counts. Load the vector database, perform a similarity search, and initialize the language model for factual answers.\n\nRetrievalQA Chain with Prompt:\nUtilize a prompt template to guide the context's use for answering questions. Initialize the RetrievalQA chain with a language model, vector database, and the prompt template. Retrieve answers to questions while considering the source documents.\n\nRetrievalQA Chain with MapReduce, Refine, and MapRerank:\nExplore different techniques for handling multiple documents. MapReduce involves individual document calls, while Refine uses sequential refinement for improved answers. Be aware of the trade-offs in speed and result quality.\n\nRetrievalQA Limitations:\nUnderstand the limitations of the RetrievalQA chain, particularly its inability to preserve conversational history. Without a memory concept, follow-up questions may lack context, highlighting a need for introducing memory in chatbot applications.\n\nNow that we're working with larger documents, we should be mindful of the input sequence limitations that our LLM has. \n\n#### Chain Types for document loader:\n\n- [`stuff`](https://docs.langchain.com/docs/components/chains/index_related_chains#stuffing) - Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model.\n- [`map_reduce`](https://docs.langchain.com/docs/components/chains/index_related_chains#map-reduce) - This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk).\n- [`refine`](https://docs.langchain.com/docs/components/chains/index_related_chains#refine) - This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n- [`map_rerank`](https://docs.langchain.com/docs/components/chains/index_related_chains#map-rerank) - This method involves running an initial prompt on each chunk of data, that not only tries to complete a task but also gives a score for how certain it is in its answer. The responses are then ranked according to this score, and the highest score is returned.\n\n<img src=\"https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_14.png?raw=true\" alt=\"Learning\" width=\"50%\">","metadata":{}},{"cell_type":"markdown","source":"#### Creating our Document QA LLM Chain\nWith our data now in vector form we need an LLM and a chain to take our queries and create tasks for our LLM to perform.","metadata":{}},{"cell_type":"code","source":"from langchain.llms import HuggingFacePipeline\n\n# We want to make this a retriever, so we need to convert our index.  \n# This will create a wrapper around the functionality of our vector database \n# so we can search for similar documents/chunks in the vectorstore and retrieve the results:\nretriever = vectordb.as_retriever()\n\n# This chain will be used to do QA on the document. We will need\n# 1 - A LLM to do the language interpretation\n# 2 - A vector database that can perform document retrieval\n# 3 - Specification on how to deal with this data\n\nhf_llm = HuggingFacePipeline.from_model_id(\n    model_id=\"google/flan-t5-large\",\n    task=\"text2text-generation\",\n    model_kwargs={\n#        \"temperature\": 0,\n        \"do_sample\":True,\n        \"max_length\": 2048,\n        \"cache_dir\": cache_dir,\n    },\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:47:42.32793Z","iopub.execute_input":"2024-01-02T14:47:42.328327Z","iopub.status.idle":"2024-01-02T14:47:44.764503Z","shell.execute_reply.started":"2024-01-02T14:47:42.328274Z","shell.execute_reply":"2024-01-02T14:47:44.76353Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from langchain.chains import RetrievalQA\n\nqa = RetrievalQA.from_chain_type(\n    llm=hf_llm, \n    chain_type=\"refine\", \n    retriever=retriever\n)\nquery = \"Who is the main character in the Merchant of Venice?\"\nquery_results_venice = qa.run(query)\nprint(\"#\" * 12)\nquery_results_venice","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:47:56.394062Z","iopub.execute_input":"2024-01-02T14:47:56.394481Z","iopub.status.idle":"2024-01-02T14:48:04.457046Z","shell.execute_reply.started":"2024-01-02T14:47:56.394446Z","shell.execute_reply":"2024-01-02T14:48:04.45606Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc1f5312ff9b44afb1b871085e4af97e"}},"metadata":{}},{"name":"stdout","text":"############\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'ANTONIO'"},"metadata":{}}]},{"cell_type":"code","source":"qa = RetrievalQA.from_chain_type(\n    llm=hf_llm, \n    chain_type=\"refine\", \n    retriever=retriever\n)\nquery = \"What happens to Romeo and Juliet?\"\nquery_results_romeo = qa.run(query)\nprint(\"#\" * 12)\nquery_results_romeo","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:48:07.002341Z","iopub.execute_input":"2024-01-02T14:48:07.002722Z","iopub.status.idle":"2024-01-02T14:48:24.664217Z","shell.execute_reply.started":"2024-01-02T14:48:07.00269Z","shell.execute_reply":"2024-01-02T14:48:24.663269Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482da3e2cfdf479fa3baf47ae4bd4d63"}},"metadata":{}},{"name":"stdout","text":"############\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'Management confined Romeo and Juliet at the Tower of London. Romeo is killed by Benito Mussolini. Her body was infused with poison which eventually caused her death'"},"metadata":{}}]},{"cell_type":"code","source":"qa = RetrievalQA.from_chain_type(\n    llm=hf_llm, \n    chain_type=\"refine\", \n    retriever=retriever\n)\nquery = \"Does King John die?\"\nquery_results_romeo = qa.run(query)\nprint(\"#\" * 12)\nquery_results_romeo","metadata":{"execution":{"iopub.status.busy":"2024-01-02T14:47:34.928861Z","iopub.execute_input":"2024-01-02T14:47:34.929167Z","iopub.status.idle":"2024-01-02T14:47:42.325332Z","shell.execute_reply.started":"2024-01-02T14:47:34.929141Z","shell.execute_reply":"2024-01-02T14:47:42.323563Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97acf7b44c334a59840fcbd4ed4be5e3"}},"metadata":{}},{"name":"stdout","text":"############\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'King John died 1166'"},"metadata":{}}]}]}