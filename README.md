# | NLP | LLM | LangChain | Question Answer Data |

## Natural Language Processing (NLP) and Large Language Models (LLM) with langchain for Question Answering on Own Data

![Learning](https://t3.ftcdn.net/jpg/06/14/01/52/360_F_614015247_EWZHvC6AAOsaIOepakhyJvMqUu5tpLfY.jpg)


# <b><span style='color:#78D118'>|</span> Overview</b>

In this notebook we're going to augment the knowledge base of our LLM with additional data:
- We will walk through how to load data, local text file using a `DocumentLoader`, split it into chunks, and store it in a vector database using `ChromaDB`.
- And using Question Answering on Own Data 

Inretrieval augmented generation (RAG) framework, an LLM retrieves contextual documents from an external dataset as part of its execution. This is useful when we want to ask questions about specific documents (e.g., PDFs, videos, etc). 

<img src="https://github.com/YanSte/NLP-LLM-LangChain-Question-Answer-Data/blob/main/img_1.png?raw=true" alt="Learning" width="100%">


## Learning Objectives

 By the end of this notebook, you will be able to:
1. Add external local data to your LLM's knowledge base via a vector database.
2. Construct a Question-Answer(QA) LLMChain to "talk to your data."
3. Load external data sources from remote locations and store in a vector database.
4. Leverage different retrieval methods to search over your data. 

<img src="https://deepsense.ai/wp-content/uploads/2023/10/LangChain-announces-partnership-with-deepsense.jpeg" alt="Learning" width="50%">


[Using-langchain-for-question-answering-on-own-data](https://medium.com/@onkarmishra/using-langchain-for-question-answering-on-own-data-3af0a82789ed)
